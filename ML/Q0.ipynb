{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)\n",
    "X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data for algorithms that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"=== REGRESSION MODELS ===\")\n",
    "\n",
    "# Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_reg, y_train_reg)\n",
    "print(f\"Linear Regression R²: {lin_reg.score(X_test_reg, y_test_reg):.4f}\")\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0, random_state=42)  # Most important: alpha\n",
    "ridge.fit(X_train_reg, y_train_reg)\n",
    "print(f\"Ridge Regression R²: {ridge.score(X_test_reg, y_test_reg):.4f}\")\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=0.1, random_state=42, max_iter=1000)  # Most important: alpha\n",
    "lasso.fit(X_train_reg, y_train_reg)\n",
    "print(f\"Lasso Regression R²: {lasso.score(X_test_reg, y_test_reg):.4f}\")\n",
    "\n",
    "# KNN Regression\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5, weights='uniform', metric='minkowski')\n",
    "knn_reg.fit(X_train_reg, y_train_reg)\n",
    "print(f\"KNN Regression R²: {knn_reg.score(X_test_reg, y_test_reg):.4f}\")\n",
    "\n",
    "# Decision Tree Regression\n",
    "tree_reg = DecisionTreeRegressor(max_depth=5, min_samples_split=5, random_state=42)\n",
    "tree_reg.fit(X_train_reg, y_train_reg)\n",
    "print(f\"Decision Tree Regression R²: {tree_reg.score(X_test_reg, y_test_reg):.4f}\")\n",
    "\n",
    "print(\"\\n=== CLASSIFICATION MODELS ===\")\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(C=1.0, penalty='l2', random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"Logistic Regression Accuracy: {log_reg.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# KNN Classification\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='minkowski')\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "print(f\"KNN Classification Accuracy: {knn_clf.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# SVM Classification\n",
    "svm_clf = SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42, probability=True)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "print(f\"SVM Classification Accuracy: {svm_clf.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# Naive Bayes\n",
    "nb_clf = GaussianNB(var_smoothing=1e-9)\n",
    "nb_clf.fit(X_train_scaled, y_train)\n",
    "print(f\"Naive Bayes Accuracy: {nb_clf.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# LDA\n",
    "lda = LinearDiscriminantAnalysis(solver='svd')\n",
    "lda.fit(X_train_scaled, y_train)\n",
    "print(f\"LDA Accuracy: {lda.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# Decision Tree Classification\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "print(f\"Decision Tree Accuracy: {tree_clf.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# AdaBoost\n",
    "ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "print(f\"AdaBoost Accuracy: {ada.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "print(f\"Gradient Boosting Accuracy: {gb.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Stacking Ensemble\n",
    "base_learners = [\n",
    "    ('log_reg', LogisticRegression(C=1.0, random_state=42)),\n",
    "    ('svm', SVC(C=1.0, kernel='linear', probability=True, random_state=42)),\n",
    "    ('tree', DecisionTreeClassifier(max_depth=3, random_state=42))\n",
    "]\n",
    "\n",
    "stack_clf = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    passthrough=False  # Set to True for passthrough stacking\n",
    ")\n",
    "stack_clf.fit(X_train_scaled, y_train)\n",
    "print(f\"Stacking Ensemble Accuracy: {stack_clf.score(X_test_scaled, y_test):.4f}\")"
   ],
   "id": "23e1873cbd0e94b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#regression outlier detection\n",
    "def detect_outliers_iqr(data):\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (data < lower_bound) | (data > upper_bound)\n",
    "\n",
    "target_outliers_iqr = detect_outliers_iqr(y)\n",
    "print(f\"IQR outliers found: {np.sum(target_outliers_iqr)}\")\n",
    "\n",
    "def remove_regression_outliers(X, y, method='cooks', threshold=3):\n",
    "    \"\"\"\n",
    "    Remove outliers from regression data\n",
    "    Methods: 'zscore', 'iqr', 'cooks', 'residual'\n",
    "    \"\"\"\n",
    "    X_clean, y_clean = X.copy(), y.copy()\n",
    "\n",
    "    if method == 'zscore':\n",
    "        outliers = detect_outliers_zscore(y, threshold)\n",
    "    elif method == 'iqr':\n",
    "        outliers = detect_outliers_iqr(y)\n",
    "    elif method == 'residual':\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        residuals = y - model.predict(X)\n",
    "        outliers = detect_outliers_zscore(residuals, threshold)\n",
    "    elif method == 'cooks':\n",
    "        X_with_const = sm.add_constant(X)\n",
    "        model_sm = sm.OLS(y, X_with_const).fit()\n",
    "        influence = OLSInfluence(model_sm)\n",
    "        cooks_d = influence.cooks_distance[0]\n",
    "        outliers = cooks_d > 4 / len(X)\n",
    "\n",
    "    # Remove outliers\n",
    "    X_clean = X[~outliers]\n",
    "    y_clean = y[~outliers]\n",
    "\n",
    "    print(f\"Removed {np.sum(outliers)} outliers using {method} method\")\n",
    "    print(f\"Original shape: {X.shape}, Cleaned shape: {X_clean.shape}\")\n",
    "\n",
    "    return X_clean, y_clean, outliers\n",
    "\n",
    "# Usage example\n",
    "X_clean, y_clean, outliers = remove_regression_outliers(X, y, method='iqr')"
   ],
   "id": "d6dedb761c960918"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#outlier analysis for classification problem\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate classification data\n",
    "X_clf, y_clf = make_classification(n_samples=300, n_features=2, n_redundant=0,\n",
    "                                   n_clusters_per_class=1, random_state=42)\n",
    "# Add outliers\n",
    "X_clf[50] = [4, 4]   # Outlier in class 0\n",
    "X_clf[150] = [-4, -4] # Outlier in class 1\n",
    "y_clf = y_clf.astype(int)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Box plot for each feature by class\n",
    "for i in range(X_clf.shape[1]):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    data_by_class = [X_clf[y_clf == j][:, i] for j in np.unique(y_clf)]\n",
    "    plt.boxplot(data_by_class, labels=[f'Class {j}' for j in np.unique(y_clf)])\n",
    "    plt.title(f'Feature {i+1} - Box Plot by Class')\n",
    "    plt.ylabel('Feature Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "################\n",
    "def remove_classification_outliers(X, y, method='isolation_forest', **kwargs):\n",
    "    \"\"\"\n",
    "    Remove outliers from classification data\n",
    "    Methods: 'isolation_forest', 'lof', 'dbscan', 'classwise_iqr'\n",
    "    \"\"\"\n",
    "    X_clean, y_clean = X.copy(), y.copy()\n",
    "\n",
    "    if method == 'isolation_forest':\n",
    "        contamination = kwargs.get('contamination', 0.05)\n",
    "        outliers = detect_outliers_isolation_forest(X, contamination)\n",
    "    elif method == 'lof':\n",
    "        contamination = kwargs.get('contamination', 0.05)\n",
    "        outliers = detect_outliers_lof(X, contamination)\n",
    "    elif method == 'dbscan':\n",
    "        eps = kwargs.get('eps', 0.5)\n",
    "        min_samples = kwargs.get('min_samples', 5)\n",
    "        outliers = detect_outliers_dbscan(X, eps, min_samples)\n",
    "    elif method == 'classwise_iqr':\n",
    "        outliers = detect_classification_outliers(X, y, method='iqr')\n",
    "\n",
    "    # Remove outliers\n",
    "    X_clean = X[~outliers]\n",
    "    y_clean = y[~outliers]\n",
    "\n",
    "    print(f\"Removed {np.sum(outliers)} outliers using {method} method\")\n",
    "    print(f\"Original shape: {X.shape}, Cleaned shape: {X_clean.shape}\")\n",
    "\n",
    "    # Check class distribution after cleaning\n",
    "    unique, counts = np.unique(y_clean, return_counts=True)\n",
    "    print(f\"Class distribution after cleaning: {dict(zip(unique, counts))}\")\n",
    "\n",
    "    return X_clean, y_clean, outliers\n",
    "\n",
    "# Usage examples\n",
    "X_clf_clean, y_clf_clean, clf_outliers = remove_classification_outliers(\n",
    "    X_clf, y_clf, method='isolation_forest', contamination=0.05\n",
    ")"
   ],
   "id": "7c28a858b348aff2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#KFold\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=25)  #KFold\n",
    "results = cross_val_score(lr, X, y, cv=kfold) # defaults to accuracy\n",
    "print( results.mean() )\n",
    "\n",
    "\n",
    "#grid search CV\n",
    "params = {'solver': ['lbfgs','liblinear','newton-cg','newton-cholesky','sag','saga'],\n",
    "          'C': np.linspace(0.001, 4, 20)}\n",
    "gcv = GridSearchCV(lr, param_grid=params, cv=kfold, scoring='roc_auc')"
   ],
   "id": "de7508b786fd992c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Bagging,Boosting,RandomForest\n",
    "est_list = [nb, dtc, knn, lr]\n",
    "n_est = [10, 15, 25, 50]\n",
    "scores = []\n",
    "for e in tqdm(est_list):\n",
    "    for n in n_est:\n",
    "        bagg = BaggingClassifier(random_state=25, n_estimators=n, estimator=e )\n",
    "        bagg.fit(X_train_trns, y_train)\n",
    "        y_pred_prob = bagg.predict_proba(X_test_trns)\n",
    "        scores.append([e, n, log_loss(y_test, y_pred_prob)])\n",
    "df_scores = pd.DataFrame( scores, columns=['Estimator','B-Samples','score'] )\n",
    "df_scores.sort_values('score')\n",
    "\n",
    "gbm = GradientBoostingClassifier(random_state=25,n_estimators=n,\n",
    "                                             max_depth=d, learning_rate=r)\n",
    "\n",
    " rf = RandomForestClassifier(random_state=25, max_features=f)"
   ],
   "id": "6e926394fe76f629"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#stack ensembling\n",
    "voting = VotingClassifier(estimators=[('TREE',dtc),('KNN',knn),('NB',nb)])"
   ],
   "id": "158c3acdf7d0fff1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#clustering\n",
    "wss = []\n",
    "for i in range(2, 11):\n",
    "    clust = KMeans(random_state=25, n_clusters=i)\n",
    "    clust.fit(milk_scaled)\n",
    "    wss.append([i, clust.inertia_] )                                               #silhouette_score(milk_scaled,clust.labels_)]\n",
    "df_wss = pd.DataFrame( wss, columns=['clusters','wss'] )\n",
    "plt.scatter(df_wss['clusters'], df_wss['wss'])\n",
    "plt.plot( df_wss['clusters'], df_wss['wss'] )\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"WSS\")\n",
    "plt.title(\"Scree Plot\")\n",
    "\n",
    "\n",
    "\n",
    "epsilons = [0.2, 0.4, 0.6, 0.8, 1, 1.2]\n",
    "min_pcts = [2, 3, 4, 5]\n",
    "scores = []\n",
    "for e in epsilons:\n",
    "    for m in min_pcts:\n",
    "        clust = DBSCAN(eps=e, min_samples=m)\n",
    "        clust.fit(milk_scaled)\n",
    "        inliers = milk_scaled.copy()\n",
    "        inliers['label'] = clust.labels_\n",
    "        inliers = inliers[inliers['label']!=-1]\n",
    "        # len( np.unique( inliers['label'] )) are the number of clusters\n",
    "        # getting formed\n",
    "        if len( np.unique( inliers['label'] )) >= 2:\n",
    "            scores.append([e, m, silhouette_score(inliers.iloc[:,:-1], inliers['label'])])\n",
    "df_scores = pd.DataFrame(scores, columns=['eps','min','score'])\n",
    "df_scores.sort_values('score', ascending=False)"
   ],
   "id": "98bdc946091c76f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#pca\n",
    "model = pca()\n",
    "results = model.fit_transform(milk_scaled,\n",
    "                              col_labels=milk.columns,\n",
    "                              row_labels=list(milk.index))\n",
    "model.biplot(label=True,legend=True)\n",
    "for i in np.arange(0, milk.shape[0] ):\n",
    "    plt.text(pc_data.values[i,0],\n",
    "             pc_data.values[i,1],\n",
    "             list(milk.index)[i])\n",
    "plt.show()"
   ],
   "id": "24112e6c381df0c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Time Series",
   "id": "1a604452fdf6c64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "series = df['Milk']\n",
    "result = seasonal_decompose(series, model='multiplicative',period=12)\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "y = df['Milk']\n",
    "fcast = y.rolling(3,center=True).mean()\n",
    "plt.plot(y, label='Original Data')\n",
    "plt.plot(fcast, label='Centered Moving Average')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#trailing\n",
    "y_train = df['Milk'].iloc[:-12]\n",
    "y_test = df['Milk'].iloc[-12:]\n",
    "span=5\n",
    "fcast = y_train.rolling(span).mean()\n",
    "MA = fcast.iloc[-1]\n",
    "MA_series = pd.Series(MA.repeat(len(y_test)))\n",
    "MA_fcast = pd.concat([fcast,MA_series], ignore_index=True)\n",
    "rmse = root_mean_squared_error(y_test, MA_series)\n",
    "plt.plot(y_train, label='Train')\n",
    "plt.plot(y_test, label='Test')\n",
    "plt.plot(MA_fcast, label='Rolling Average Forecast')\n",
    "plt.title(f\"RMSE = {rmse:.2f}\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#simple exponential smoothing\n",
    "alpha = 0.2\n",
    "ses = SimpleExpSmoothing(y_train)\n",
    "fit1 = ses.fit(smoothing_level=alpha)\n",
    "fcast1 = fit1.forecast(len(y_test))\n",
    "y_test.plot(color=\"pink\", label='Test')\n",
    "fcast1.plot(color=\"purple\", label='Forecast')\n",
    "rmse = root_mean_squared_error(y_test, fcast1)\n",
    "plt.title(f\"RMSE = {rmse:.2f}\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "id": "22232aa1613e604f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#holt linear\n",
    "alpha = 0.8\n",
    "beta = 0.02\n",
    "holt = Holt(y_train)\n",
    "fit1 = holt.fit(smoothing_level=alpha, smoothing_trend=beta)\n",
    "fcast1 = fit1.forecast(len(y_test))\n",
    "y_test.plot(color=\"pink\", label='Test')\n",
    "fcast1.plot(color=\"purple\", label='Forecast')\n",
    "rmse = root_mean_squared_error(y_test, fcast1)\n",
    "plt.title(f\"RMSE = {rmse:.2f}\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#holt exp\n",
    "holt = Holt(y_train, exponential=True)\n",
    "def holt_exp(alpha, beta):\n",
    "    fit1 = holt.fit(smoothing_level=alpha, smoothing_trend=beta)\n",
    "    fcast1 = fit1.forecast(len(y_test))\n",
    "    y_test.plot(color=\"pink\", label='Test')\n",
    "    fcast1.plot(color=\"purple\", label='Forecast')\n",
    "    rmse = root_mean_squared_error(y_test, fcast1)\n",
    "    plt.title(f\"RMSE = {rmse:.2f}, alpha = {alpha:.2f}, beta = {beta:.2f}\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "widgets.interact(holt_exp, alpha=(0.01, 1, 0.01), beta=(0.01, 1, 0.01))\n",
    "\n",
    "\n",
    "#damped meth\n",
    "def damped(alpha, beta, phi, exponentiality, dampness):\n",
    "    holt = Holt(y_train, exponential=exponentiality, damped_trend=dampness)\n",
    "    fit1 = holt.fit(smoothing_level=alpha, smoothing_trend=beta, damping_trend=phi)\n",
    "    fcast1 = fit1.forecast(len(y_test))\n",
    "    y_test.plot(color=\"pink\", label='Test')\n",
    "    fcast1.plot(color=\"purple\", label='Forecast')\n",
    "    rmse = root_mean_squared_error(y_test, fcast1)\n",
    "    plt.title(f\"RMSE = {rmse:.2f}, alpha = {alpha:.2f}, beta = {beta:.2f}\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "widgets.interact(damped, alpha=(0.01, 1, 0.01), beta=(0.01, 1, 0.01),phi=(0.01, 1, 0.01),\n",
    "                 exponentiality=[True, False], dampness=[True, False])\n",
    "\n",
    "\n",
    "#holt winter\n",
    "def hw(alpha, beta, gamma, seasonality, periods=12):\n",
    "    holt = ExponentialSmoothing(y_train, trend='add', seasonal=seasonality,seasonal_periods=periods)\n",
    "    fit1 = holt.fit(smoothing_level=alpha, smoothing_trend=beta, smoothing_seasonal=gamma)\n",
    "    fcast1 = fit1.forecast(len(y_test))\n",
    "    y_test.plot(color=\"pink\", label='Test')\n",
    "    fcast1.plot(color=\"purple\", label='Forecast')\n",
    "    rmse = root_mean_squared_error(y_test, fcast1)\n",
    "    plt.title(f\"RMSE={rmse:.2f}, alpha={alpha:.2f}, beta={beta:.2f}, gamma={gamma:.2f}\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "widgets.interact(hw, alpha=(0.01, 1, 0.01), beta=(0.01, 1, 0.01),gamma=(0.01, 1, 0.01),\n",
    "                 seasonality=['add', 'mul'])"
   ],
   "id": "c38e823dee1a60d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#dickey fuller\n",
    "ord_1_diff = df['Value'].diff() # does first order differencing\n",
    "ord_1_diff = ord_1_diff.dropna()\n",
    "result = adfuller(ord_1_diff, maxlag=10)\n",
    "print(\"P-Value =\", result[1])\n",
    "if result[1] < 0.05:\n",
    "    print(\"Time Series is Stationary\")\n",
    "else:\n",
    "    print(\"Time Series is not Stationary\")"
   ],
   "id": "4f997ab505dd0ebc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"FRED-NROUST.csv\")\n",
    "y_train = df['Value'].iloc[:-8]\n",
    "y_test = df['Value'].iloc[-8:]\n",
    "y_train.shape, y_test.shape\n",
    "\n",
    "model = ARIMA(y_train,order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "#print('Coefficients: %s' % model_fit.params)\n",
    "predictions = model_fit.predict(start=len(y_train), end=len(y_train)+len(y_test)-1)\n",
    "y_test.plot(color=\"pink\", label='Test')\n",
    "predictions.plot(color=\"purple\", label='Forecast')\n",
    "rmse = root_mean_squared_error(y_test, predictions)\n",
    "plt.title(f\"RMSE={rmse:.5f}\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#seasonal ARIMA\n",
    "def sarima(p,d,q, P, D, Q, S):\n",
    "    model = ARIMA(y_train,order=(p,d,q),seasonal_order=(P, D, Q, S))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(y_train), end=len(y_train)+len(y_test)-1)\n",
    "    y_test.plot(color=\"pink\", label='Test')\n",
    "    predictions.plot(color=\"purple\", label='Forecast')\n",
    "    rmse = root_mean_squared_error(y_test, predictions)\n",
    "    plt.title(f\"RMSE={rmse:.5f}\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "widgets.interact( sarima, p=(0,5,1), d=(0,5,1), q=(0,5,1),\n",
    "                 P=(0,5,1), D=(0,5,1), Q=(0,5,1) , S=12 )"
   ],
   "id": "6e37606521d25e29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#assoc rules\n",
    "fp_df = pd.read_csv('Faceplate.csv',index_col=0)\n",
    "fp_df.head()\n",
    "fp_df = fp_df.astype(bool) # convert to boolean for compatibility of apriori( )\n",
    "itemsets = apriori(fp_df, min_support=0.2, use_colnames=True)\n",
    "rules = association_rules(itemsets, metric='confidence',  min_threshold=0.6)\n",
    "rules = rules[['antecedents','consequents','support', 'confidence','lift']]\n",
    "rules\n",
    "def create_rules(min_supp, conf_thres):\n",
    "    itemsets = apriori(fp_df, min_support=min_supp, use_colnames=True)\n",
    "    rules = association_rules(itemsets, metric='confidence',  min_threshold=conf_thres)\n",
    "    rules = rules[['antecedents','consequents','support', 'confidence','lift']]\n",
    "    return rules\n",
    "widgets.interact(create_rules, min_supp=(0.01, 1, 0.01),\n",
    "                 conf_thres=(0.01, 1, 0.01))"
   ],
   "id": "a897e43e84314f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "36b536bbe6aba00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "52e18b542d9bca37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9b8a52fb3ad132e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
